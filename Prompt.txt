# MAESTRO Hierarchical Token Implementation Prompt

## Overview

Implement a complete MIDI-to-Pedal transformer codebase using **hierarchical probabilistic tokens**. This is a full rewrite with a new token architecture.

**Goal**: Build data processing, training, and inference infrastructure (NO model instantiation code).

**Location**: `C:\Users\edgar\Documents\Studium\Mathe\Bachelor\Code`

**Old Codebase**: `C:\Users\edgar\Documents\Studium\Mathe\Bachelor\Playground\MAESTRO\` (for reference)

## Quick Reference

**What you're building**: Encoder-decoder transformer that predicts segment tokens as (type, height, amount, time) 4-tuples.

**Key innovation**: Instead of discrete token classes, each segment attribute is predicted as a **distribution**:
- Stop score: Sigmoid + MSE loss
- Height: Beta(α, β) on [0,1]
- Amount: Beta(α, β) on [0,1]
- Time: dt

**File structure**: `data.py`, `model.py`, `train.py`, `inference.py`, `utils.py`, `preprocessing.py`

**Reuse from old code**: RDP simplification, curve fitting (segment class), dataset loading

---

## What NOT to Implement

**Do NOT write**:
- Model hyperparameter definitions (d_model, num_heads, num_layers, etc.)
- Model instantiation code
- Optimizer configuration
- Training script execution

**User will handle**: Model architecture configuration and training execution.

---

## Token Architecture

### Hierarchical SegmentToken Structure
Each regular segment token is a **4-tuple**: `(type, height, amount, time)`

**Type field usage**:
- **For embedding** (during teacher forcing): Type ∈ {0=SOS, 1=EOS, 2=regular segment, 3=first segment}
- **For generation**: Model outputs "stop score" (continuous) to decide stop/continue

**Model outputs 7 values via 4 heads**:
1. **Stop head** (1D): Outputs single scalar → squish to [0,1] with sigmoid
   - NOT called "type head" (confusing)
   - Loss: **MSE** with target = 1.0 if next is EOS, 0.0 if next is segment
   - Inference: Sample from Bernoulli(stop_score) to decide whether to stop

2. **Height head** (2D): Outputs (α, β) → Beta distribution on [0,1]
   - Represents segment y-value (renamed from "value" to avoid confusion)

3. **Amount head** (2D): Outputs (α, β) → Beta distribution on [0,1]
   - Represents segment curve shape parameter
   - **Post-processing**: Create separate `scale_amount()` function to transform [0,1] → custom range (user will customize later)

4. **Time head** (2D): Outputs (h1, h2) → LogNormal distribution
   - `μ = h1` (unconstrained)
   - `σ = softplus(h2) + epsilon` (ensures σ > 0)
   - Represents delta-time between segments

### Note Representation
**Only Note Class**. Each note is encoded as a single vector:
- Concatenate: `[start_embed, duration_embed, pitch_embed, velocity_embed]`
- No type embedding needed (notes are homogeneous)

---

## Model Architecture

### Encoder-Decoder Transformer
- **Encoder**: Processes sequence of Notes
- **Decoder**: Autoregressively generates SegmentTokens with teacher forcing

### Embeddings

**Note Embedding (Encoder input)**:
- Each of start/duration/pitch/velocity gets separate embedding
- Concatenate all 4 → produces encoder input vector
- Add standard positional encoding
- Reuse TimeEmbedding class from old code/cl

**Segment Embedding (Decoder input during training with teacher forcing)**:

- **If type = 0 or 1** (SOS, EOS): Use `nn.Embedding` lookup
- **If type = 2** (normal segment):
  - Linear projection for height → d_model/3
  - Linear projection for amount → d_model/3
  - Linear projection for time → d_model/3
  - Concatenate all 3 parts
- **If type = 3**: Use learnable vector for amount. (Explanation: The first segment needs two tokens to be represented, rendering amount a useless value for the first token.)

### Output Heads
Four separate heads from decoder hidden state:
1. `stop_head`: Linear → 1D output → apply sigmoid
2. `height_head`: Linear → 2D output (α, β params)
3. `amount_head`: Linear → 2D output (α, β params)
4. `time_head`: Linear → 1D output (h1, h2)

**Transformations**:
- Stop: `sigmoid(stop_head_output)` → [0, 1]
- Height/Amount: `softplus(params)` to ensure α, β > 0
- Time sigma: `softplus(h2) + epsilon`

---

## Loss Function

**Combined loss** across all heads:

```python
# Stop head: MSE regression
stop_score = sigmoid(stop_head_output)
target_stop = 1.0 if next_token_is_EOS else 0.0
stop_loss = MSE(stop_score, target_stop)

# Height: Beta NLL
alpha_h, beta_h = softplus(height_head[0]), softplus(height_head[1])
height_loss = -log P_Beta(ground_truth_height | alpha_h, beta_h)

# Amount: Beta NLL
alpha_a, beta_a = softplus(amount_head[0]), softplus(amount_head[1])
amount_loss = -log P_Beta(ground_truth_amount | alpha_a, beta_a)

# Time: LogNormal NLL
mu = time_head[0]
sigma = softplus(time_head[1]) + epsilon
time_loss = -log P_LogNormal(ground_truth_dt | mu, sigma)

# Total (mask height/amount/time losses when target is EOS)
total_loss = stop_loss + mask * (height_loss + amount_loss + time_loss)
```

**Important**:
- Use `torch.distributions.Beta` and `torch.distributions.LogNormal` for proper PDFs
- **Mask** the distributional losses when target is EOS (only stop_loss applies)
- **Mask** amount loss for first token (type=3).
- Handle Beta edge cases: if ground_truth is exactly 0 or 1, add small epsilon

---

## Data Processing

### Preprocessing Pipeline (REUSE from old codebase)

Reference: `C:\Users\edgar\Documents\Studium\Mathe\Bachelor\Playground\MAESTRO\create_segments.py`

**Pipeline** create_dataset function:
1. Load MIDI files from `maestro-v3.0.0/` dataset via get_paths
2. Extract pedal CC#64 events → PedalEvent list via get_track
3. Apply RDP simplification (epsilon=0.12) via pedals_to_segs
4. Fit curves using internal functions of pedals_to_segs
5. Extract notes → NoteEvent list
6. Augment data via augment function
7. Create SegmentToken sequences from fitted segments via tokenize_segs
8. Save to dataset file using safe_to_pkl

**Key functions to reuse**:
1. `rdp_simplify(time, value, epsilon)` - algorithmically determines where to place segments
2. `segment` class - Curve functions (Single, Double, Plateau)
3. `FitHandler.fit_curve()` - Fits curves using scipy optimization

**Important Note:**
Don't import anything from the old project. Instead, copy the core functional blocks, but keep to the new processing pipeline.
pedals_to_segs is to be written entirely from old code. The return type SegmentEvent is the raw representation that this project is trying to learn. This has not changed from before.

### Data Augmentation

**Modified transposition logic**:
- Sample transposition values in range [-12, +12] semitones
- **Track used values**: Maintain set of tried transpositions per track
- **Out-of-bounds handling**:
  - If transposition causes pitch out of range [0, 87]:
    - Mark all higher/lower values as unavailable (depending on direction)
    - Sample new value from remaining valid range
  - Stop augmenting when no valid transpositions remain OR target count reached
- Keep time stretching: 0.9x to 1.1x random

### Dataset Class

**10-second windowing**:
- Cannot copy old `extract_window` implementation (different token grammar)
- Implement new version:
  - Extract all notes and segment tokens in [t, t+10]
  - Turn the first token of the sequence into a type=3 (This token of type 3 encodes the start time and height of the first segment in the window)
  - Prepend SOS and append EOS to the segment tokens

The window extraction now happens inside call(track, time) of Dataset
call() samples randomly a track and time and calls call(track, time). call() is used for training.

### Inference notes:
- Make the model output at least TWO tokens, or sequence is degenerate. -> Only start using stop head to decide sequence end after second token is generated.
- Use the first two tokens to decode the first segment. Ignore amount of the first token. Use its time and height for the start time and height of the first segment
- The start times and heights of the regular segments is equal to the end times and heights of the previous segment. (As in old code decoding strategy)

---

### Visualization

**Use Plotly with scrollable time axis** (reference: `Custom_Interpolation.ipynb`):

```python
import plotly.graph_objects as go

fig = go.Figure()
# Add traces for ground truth and generated pedal curves
fig.update_xaxes(rangeslider_visible=True) # this enables scrolling
fig.update_layout(title="...", xaxis_title="Time (s)", yaxis_title="Pedal Value")
fig.show()
```

**Decoding segments to curves**:
- Convert SegmentTokens → segment objects (from create_segments.py)
  - **IMPORTANT**: Always use `curve_type='single'` for now
  - Copy the entire `segment` class from old codebase for future extensibility
  - Create `scale_amount(beta_sample)` function to transform amount from [0,1] to appropriate range (user will customize)
- Evaluate segment curves at dense time points (100 samples)
- Plot overlaid curves for comparison

---

## File Structure

Use **traditional ML organization**:

```
data.py           # All Data Classes
model.py          # All Model Classes
train.py          # step, train
inference.py      # generate, tokens_to_segs
utils.py          # plot, safe_to_pkl
preprocessing.py  # create_dataset, pedals_to_segs, tokenize_segs, augment
```
feel free to move the functions around a bit in case one file (especially preprocessing) becomes too long.


---

## Critical Implementation Notes

### From Old Codebase

**Directly reusable**:
- `create_segments.py`: All curve fitting logic (RDP, segment class, FitHandler)
- Preprocessing pipeline structure from `Preprocessor.py`
- MAESTRO dataset loading logic

**NOT reusable**:
- Tokenization (entirely different token grammar)
- Old model architectures (CC/CT/TC/TT)
- Old training loss (was cross-entropy on discrete buckets)

### Segment Curve Math

**IMPORTANT** (from old code comment in `FitHandler._objective`):
```python
# AT CLAUDE CODE: THE ERROR IS ALREADY RAISED TO THE POWER OF P
total_error += error  # Don't raise again!
return total_error ** (1/p)  # Final Lp-norm calculation
```

### Code Quality Guidelines

1. **No try-catch blocks**: Let code fail loudly (user preference)
2. **Minimal comments**: Only document non-obvious logic
3. **Function decomposition**: Feel free to split large functions beyond the LucidChart structure
4. **No debugging names**: When fixing bugs, keep original function names (don't create `processor_corrected`)

---

## References

- **Old codebase**: `C:\Users\edgar\Documents\Studium\Mathe\Bachelor\Playground\MAESTRO\`
- **Dataset**: `maestro-v3.0.0/` (metadata: `maestro-v3.0.0.json`)
- **Plotting example**: `Playground/MAESTRO/Playground/Custom_Interpolation.ipynb`
- **LucidChart**: User-provided architecture diagram (4 modules: Data, Model, Preprocessing, Training/Inference)

---

**All code should be production-ready, well-structured, and follow the hierarchical token paradigm exactly as specified.**
