The goal for this project is to use a transformer architecture to "translate" a sequence of MIDI notes into a sequence of MIDI pedal events.

I want to compare four different methods for tokenizing notes and pedal segments (a segment is what I call a custom piece of function I used to fit the individual
pedal events in order to reduce the amount of data), specifically whether it's more beneficial to use time tokens or whether to embed time continuously.

The non-time tokens are fixed: 
- 33 Velocity, 88 Note on and 88 Note off tokens for notes (total 209)
- 22 Value and 11 amount tokens for pedal segments (total 33)

continuous time embedding vs. tokenized time input (C vs. T)

with

continuous time decoding vs. tokenized time output (C vs. T)

total: four approaches:
CC, CT, TC, TT

Details: let the embedding dim be 256. Then:

Encoding:

C
- Tokenization example: (ON(55), t0), (Vel(90), t0), (ON(49), t1), (Vel(80), t1), (OFF(49), t2), (OFF(55), t3)
- (On + Off + Vel) embedding is learnable map : {0, …, 209} → dim 256
- Time embedding is a fixed sinusoidal map : t → dim 256
- Embedding = Note + Time

T
- Tokenization example: ON(55), T(0.0), Vel(90), ON(49), T(0.425), Vel(80), OFF(49), T(1.775), OFF(55), T(1.85)
- 10s windows with 1 Time Token every 25ms makes 4000 Tokens
- Embedding is learnable map : {0, …, 4209} → dim 256

Decoding:

C
- Neural Net : dim 256 → dim 33 + argmax
- Neural Net : dim 256 → t
- Output = token at time t
- Output example: (Val(0.1), t0), (Amt(1.0), t1), (Val(1.0), t2), (Amt(-0.2), t3)
- Target example: Val(0.1), Amt(1.0), T0, Val(1.0), Amt(-0.2), T1. Obtain T0 = (t0 + t1)/2 or T0 = t0 for example...

T
- Neural Net : dim 256 → dim 4032 + argmax
- Target example: Val(0.1), T(0.0), Amt(1.0), Val(1.0), T(0.825), Amt(-0.2)


Dataset:
MAESTRO dataset: +200 hours of MIDI piano recordings in 1200 pieces of 3 to 10 minutes each. About 8 000 notes and 500 segments per track.